# Requirements for vlm.py - Qwen3-VL Chat Interface
#
# PLATFORM NOTES:
#   - Windows: Use transformers backend only (vLLM is Linux-only)
#   - Linux: Can use vLLM for high-performance inference
#
# WINDOWS SETUP:
#   1. Install PyTorch with CUDA first:
#      pip install torch==2.7.1+cu128 torchvision==0.22.1+cu128 --index-url https://download.pytorch.org/whl/cu128
#   2. Then install requirements:
#      pip install -r requirements_vlm.txt
#   3. Run:
#      python vlm.py --backend transformers
#
# LINUX SETUP (with vLLM):
#   1. Install PyTorch with CUDA first:
#      pip install torch==2.7.1+cu128 torchvision==0.22.1+cu128 --index-url https://download.pytorch.org/whl/cu128
#   2. Install requirements:
#      pip install -r requirements_vlm.txt
#   3. Install vLLM separately:
#      pip install vllm==0.11.0
#   4. Run:
#      python vlm.py --backend vllm

# NOTE: Install torch/torchvision separately with CUDA index first!

# Qwen VL utilities (has deps: av, packaging, pillow, requests)
qwen-vl-utils>=0.0.14

# Transformers backend
# GLM-4.6V-Flash requires transformers v5.0.0rc0+ (PR #42122 added GLM-4.6V support)
# NOT available in v4.57.x stable releases!
#
# Install ONE of these options:
#   pip install transformers==5.0.0rc0      # Pre-release from PyPI
#   pip install transformers --pre          # Latest pre-release
#   pip install git+https://github.com/huggingface/transformers  # Dev (5.0.0.dev0)
#
# Source: https://github.com/huggingface/transformers/releases/tag/v5.0.0rc0
transformers==5.0.0rc0
accelerate
safetensors

# Gradio UI - 6.x required for huggingface_hub 1.x compatibility (used by transformers 5.x)
# Fix: PR #11979 - "Allow gradio to run with huggingface_hub 1.x"
gradio>=6.0.0
gradio-client>=1.5.0

# Image/Video processing (let qwen-vl-utils control these)
opencv-python

# Other dependencies
numpy<2.0.0
tqdm
pydantic>=2.0.0
huggingface-hub>=0.20.0

# vLLM (Linux only - install separately if needed):
# pip install vllm==0.11.0
